{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cpG7CV-OZAq"
      },
      "outputs": [],
      "source": [
        "# Installing core dependencies\n",
        "\n",
        "!pip install transformers[torch] datasets sentencepiece wandb pyconll\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloning repositories\n",
        "!git clone https://github.com/lgessler/microbert\n",
        "!git clone https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "nt2rZlQ-O-8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the data\n",
        "\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import pyconll\n",
        "\n",
        "LANGUAGES = ['coptic', 'english', 'greek', 'greek_old', 'indonesian',\n",
        "             'latin', 'maltese', 'tamil', 'uyghur', 'wolof', 'wolof_old']\n",
        "\n",
        "def load_conllu_data(lang: str):\n",
        "    \"\"\"Load and process UD data for target language\"\"\"\n",
        "    base_path = Path(f\"microbert/data/{lang}/UD_*/\")\n",
        "    conll_files = list(base_path.glob(\"*.conllu\"))\n",
        "\n",
        "    raw_datasets = DatasetDict({\n",
        "        split: load_dataset('conllu', data_files=str(file), split='train')\n",
        "        for file, split in zip(conll_files, ['train', 'dev', 'test'])\n",
        "    })\n",
        "\n",
        "    return raw_datasets.map(\n",
        "        lambda ex: {'text': ' '.join(ex['tokens'])},\n",
        "        batched=True,\n",
        "        remove_columns=['id', 'lemma', 'upos', 'xpos', 'feats',\n",
        "                       'head', 'deprel', 'deps', 'misc']\n",
        "    )"
      ],
      "metadata": {
        "id": "8ishSiB7PCOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distilling the Models\n",
        "\n",
        "\n",
        "from transformers import RobertaConfig, RobertaForMaskedLM, XLMRobertaConfig, XLMRobertaForMaskedLM\n",
        "\n",
        "def create_student_model(teacher_model, model_type='roberta'):\n",
        "    \"\"\"Create distilled student model\"\"\"\n",
        "    if model_type == 'roberta':\n",
        "        configuration = RobertaConfig(\n",
        "            vocab_size=teacher_model.config.vocab_size,\n",
        "            hidden_size=512,  # Reduced from 768\n",
        "            num_hidden_layers=6,  # Reduced from 12\n",
        "            num_attention_heads=8,\n",
        "            intermediate_size=2048,\n",
        "        )\n",
        "    elif model_type == 'xlm-roberta':\n",
        "        configuration = XLMRobertaConfig(\n",
        "            vocab_size=teacher_model.config.vocab_size,\n",
        "            hidden_size=768,  # Reduced from 1024\n",
        "            num_hidden_layers=8,  # Reduced from 12\n",
        "            num_attention_heads=12,\n",
        "            intermediate_size=3072,\n",
        "        )\n",
        "\n",
        "    return RobertaForMaskedLM(configuration) if model_type == 'roberta' \\\n",
        "           else XLMRobertaForMaskedLM(configuration)\n"
      ],
      "metadata": {
        "id": "Or5DkoCNPIZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Pipeline\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch.nn import KLDivLoss, MSELoss\n",
        "\n",
        "class DistillationTrainer(Trainer):\n",
        "    \"\"\"Custom trainer for knowledge distillation\"\"\"\n",
        "    def __init__(self, *args, teacher_model=None, alpha=0.5, temperature=2.0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher = teacher_model\n",
        "        self.alpha = alpha  # Weight between KL and MSE losses\n",
        "        self.temperature = temperature\n",
        "        self.kl_loss = KLDivLoss(reduction='batchmean')\n",
        "        self.mse_loss = MSELoss()\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        student_outputs = model(**inputs)\n",
        "\n",
        "        # Get teacher predictions\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = self.teacher(\n",
        "                input_ids=inputs['input_ids'],\n",
        "                attention_mask=inputs['attention_mask']\n",
        "            )\n",
        "\n",
        "        # Calculate losses\n",
        "        loss_ce = student_outputs.loss\n",
        "        loss_kl = self.kl_loss(\n",
        "            torch.log_softmax(student_outputs.logits / self.temperature, dim=-1),\n",
        "            torch.softmax(teacher_outputs.logits / self.temperature, dim=-1)\n",
        "        ) * (self.temperature ** 2)\n",
        "\n",
        "        # Hidden states MSE loss\n",
        "        loss_mse = self.mse_loss(\n",
        "            student_outputs.hidden_states[-1],\n",
        "            teacher_outputs.hidden_states[-1]\n",
        "        )\n",
        "\n",
        "        total_loss = (1 - self.alpha) * loss_ce + \\\n",
        "                     self.alpha * (0.7 * loss_kl + 0.3 * loss_mse)\n",
        "\n",
        "        return (total_loss, student_outputs) if return_outputs else total_loss\n"
      ],
      "metadata": {
        "id": "XTWx961fPVRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# workflow\n",
        "\n",
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
        "\n",
        "def run_experiment(lang: str, model_type='roberta'):\n",
        "    # Load data\n",
        "    dataset = load_conllu_data(lang)\n",
        "\n",
        "    # Initialize models\n",
        "    teacher_name = 'roberta-base' if model_type == 'roberta' else 'xlm-roberta-base'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(teacher_name)\n",
        "    teacher = AutoModelForMaskedLM.from_pretrained(teacher_name)\n",
        "    student = create_student_model(teacher, model_type)\n",
        "\n",
        "    # Tokenization\n",
        "    def tokenize_fn(examples):\n",
        "        return tokenizer(\n",
        "            examples['text'],\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_special_tokens_mask=True\n",
        "        )\n",
        "\n",
        "    tokenized_ds = dataset.map(tokenize_fn, batched=True)\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm_probability=0.15\n",
        "    )\n"
      ],
      "metadata": {
        "id": "KhFAmi0jPqpt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"results/{lang}_{model_type}\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=5e-5,\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"logs/{lang}_{model_type}\",\n",
        "        report_to=\"wandb\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ETIouTYzWJkb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train student\n",
        "trainer = DistillationTrainer(\n",
        "        model=student,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_ds[\"train\"],\n",
        "        eval_dataset=tokenized_ds[\"dev\"],\n",
        "        data_collator=data_collator,\n",
        "        teacher_model=teacher,\n",
        "        alpha=0.7,\n",
        "        temperature=4.0\n",
        "    )\n",
        "\n",
        "trainer.train()\n",
        ""
      ],
      "metadata": {
        "id": "fpyIjyOhWOmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Save final model\n",
        "student.save_pretrained(f\"models/{lang}_{model_type}_distilled\")\n",
        "tokenizer.save_pretrained(f\"models/{lang}_{model_type}_distilled\")"
      ],
      "metadata": {
        "id": "F01OKlcvWTxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(lang: str, model_type='roberta'):\n",
        "    # Load test data\n",
        "    test_ds = load_conllu_data(lang)['test']\n",
        "\n",
        "    # Initialize models\n",
        "    model_path = f\"models/{lang}_{model_type}_distilled\"\n",
        "    model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    # Perplexity calculation\n",
        "    eval_results = trainer.evaluate(tokenized_ds[\"test\"])\n",
        "    perplexity = np.exp(eval_results[\"eval_loss\"])\n",
        "\n",
        "    # Masked prediction accuracy\n",
        "    fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
        "\n",
        "    # Create masked examples\n",
        "    masked_examples = []\n",
        "    for example in test_ds.shuffle().select(range(100)):\n",
        "        tokens = example['text'].split()\n",
        "        mask_pos = np.random.randint(0, len(tokens))\n",
        "        tokens[mask_pos] = tokenizer.mask_token\n",
        "        masked_examples.append(' '.join(tokens))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct = 0\n",
        "    for example, masked in zip(test_ds.select(range(100)), masked_examples):\n",
        "        predictions = fill_mask(masked)\n",
        "        if any(pred['token_str'] == example['text'].split()[mask_pos]\n",
        "               for pred in predictions):\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / 100\n",
        "\n",
        "    return {\n",
        "        'perplexity': perplexity,\n",
        "        'masked_accuracy': accuracy\n",
        "    }\n"
      ],
      "metadata": {
        "id": "N8nruWtgWUUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "results = []\n",
        "\n",
        "for lang in LANGUAGES:\n",
        "    # Run for Roberta\n",
        "    run_experiment(lang, 'roberta')\n",
        "    roberta_metrics = evaluate_model(lang, 'roberta')\n",
        "\n",
        "    # Run for XLM-R\n",
        "    run_experiment(lang, 'xlm-roberta')\n",
        "    xlmr_metrics = evaluate_model(lang, 'xlm-roberta')\n",
        "\n",
        "    # Get MicroBERT baseline (hypothetical values)\n",
        "    microbert_metrics = {\n",
        "        'perplexity': 15.2,  # Placeholder values - need actual baseline\n",
        "        'masked_accuracy': 0.62\n",
        "    }\n",
        "\n",
        "    results.append({\n",
        "        'language': lang,\n",
        "        'roberta_perplexity': roberta_metrics['perplexity'],\n",
        "        'xlmr_perplexity': xlmr_metrics['perplexity'],\n",
        "        'microbert_perplexity': microbert_metrics['perplexity'],\n",
        "        'roberta_accuracy': roberta_metrics['masked_accuracy'],\n",
        "        'xlmr_accuracy': xlmr_metrics['masked_accuracy'],\n",
        "        'microbert_accuracy': microbert_metrics['masked_accuracy']\n",
        "    })\n"
      ],
      "metadata": {
        "id": "yMWK9Ai9Wnyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create results dataframe\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Generate visualizations\n",
        "plt.figure(figsize=(12, 6))\n",
        "df.set_index('language')[['roberta_perplexity', 'xlmr_perplexity', 'microbert_perplexity']].plot.bar()\n",
        "plt.title('Model Comparison by Language Perplexity')\n",
        "plt.ylabel('Perplexity (lower is better)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QEV_26LpWs2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}