{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers datasets conllu torch huggingface_hub"
      ],
      "metadata": {
        "id": "MTQzCBTu_jxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaConfig, XLMRobertaConfig, Trainer, TrainingArguments\n",
        "from transformers import RobertaForMaskedLM, XLMRobertaForMaskedLM\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(token=\"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\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fy0SOaUe_jJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e5W577v9BncN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MicroBERT configuration\n",
        "MICROBERT_CONFIG = {\n",
        "    \"hidden_size\": 100,\n",
        "    \"num_hidden_layers\": 3,\n",
        "    \"num_attention_heads\": 5,\n",
        "    \"intermediate_size\": 400,\n",
        "    \"max_position_embeddings\": 512,\n",
        "    \"vocab_size\": 32000  # Adjust based on actual tokenizer\n",
        "}\n",
        "\n",
        "def create_student_model(teacher_model):\n",
        "    \"\"\"Create student model matching MicroBERT architecture\"\"\"\n",
        "    if \"xlm\" in teacher_model.config.model_type:\n",
        "        config = XLMRobertaConfig(**MICROBERT_CONFIG)\n",
        "        student = XLMRobertaForMaskedLM(config)\n",
        "    else:\n",
        "        config = RobertaConfig(**MICROBERT_CONFIG)\n",
        "        student = RobertaForMaskedLM(config)\n",
        "    return student"
      ],
      "metadata": {
        "id": "ojoijWWTBgla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distillation training setup\n",
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher = teacher_model\n",
        "        self.teacher.eval()\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        student_output = model(**inputs)\n",
        "        with torch.no_grad():\n",
        "            teacher_output = self.teacher(**inputs)\n",
        "\n",
        "        # KL divergence loss\n",
        "        loss = torch.nn.functional.kl_div(\n",
        "            torch.nn.functional.log_softmax(student_output.logits, dim=-1),\n",
        "            torch.nn.functional.softmax(teacher_output.logits, dim=-1),\n",
        "            reduction=\"batchmean\")\n",
        "\n",
        "        # Add multitask losses (POS + Parsing)\n",
        "        if \"pos_labels\" in inputs:\n",
        "            pos_loss = torch.nn.functional.cross_entropy(\n",
        "                student_output.logits.view(-1, student_output.logits.size(-1)),\n",
        "                inputs[\"pos_labels\"].view(-1)\n",
        "            )\n",
        "            loss += pos_loss\n",
        "\n",
        "        if \"parse_labels\" in inputs:\n",
        "            parse_loss = torch.nn.functional.cross_entropy(\n",
        "                student_output.logits.view(-1, student_output.logits.size(-1)),\n",
        "                inputs[\"parse_labels\"].view(-1)\n",
        "            )\n",
        "            loss += parse_loss\n",
        "\n",
        "        return (loss, student_output) if return_outputs else loss"
      ],
      "metadata": {
        "id": "39IPERgUBhv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments matching paper specs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=200,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=1000,\n",
        "    logging_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    gradient_accumulation_steps=1,\n",
        "    fp16=True,\n",
        ")"
      ],
      "metadata": {
        "id": "28uc5BMXBqU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from MicroBERT repo format\n",
        "def load_microbert_dataset(language=\"wolof\"):\n",
        "    return load_dataset(\n",
        "        \"text\",\n",
        "        data_files={\n",
        "            \"train\": f\"data/{language}/mlm/train.txt\",\n",
        "            \"validation\": f\"data/{language}/mlm/dev.txt\"\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "gLQ_AOf-BylC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for Wolof\n",
        "dataset = load_microbert_dataset(\"wolof\")\n",
        "\n",
        "# Initialize teacher and student\n",
        "teacher = XLMRobertaForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n",
        "student = create_student_model(teacher)"
      ],
      "metadata": {
        "id": "4cyq1DX4BzCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer = DistillationTrainer(\n",
        "    model=student,\n",
        "    teacher_model=teacher,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "VqhgW9IYB2Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to Hugging Face Hub\n",
        "student.push_to_hub(\"microbert-xlmr-wolof\")"
      ],
      "metadata": {
        "id": "GmaYJUXvB7Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full MicroBERT architecture details\n",
        "config = RobertaConfig(\n",
        "    vocab_size=32000,\n",
        "    hidden_size=100,\n",
        "    num_hidden_layers=3,\n",
        "    num_attention_heads=5,\n",
        "    intermediate_size=400,\n",
        "    hidden_act=\"gelu\",\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    max_position_embeddings=512,\n",
        "    type_vocab_size=1,\n",
        "    initializer_range=0.02,\n",
        "    layer_norm_eps=1e-12,\n",
        "    use_cache=True,\n",
        "    pad_token_id=1,\n",
        "    bos_token_id=0,\n",
        "    eos_token_id=2,\n",
        ")\n"
      ],
      "metadata": {
        "id": "RwQ1pnG-B7qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LANGUAGES = [\"wolof\", \"coptic\", \"maltese\", \"uyghur\", \"tamil\", \"indonesian\"]\n",
        "\n",
        "for lang in LANGUAGES:\n",
        "    dataset = load_microbert_dataset(lang)\n",
        "    student = create_student_model(teacher)\n",
        "    trainer = DistillationTrainer(...)\n",
        "    trainer.train()\n",
        "    student.push_to_hub(f\"microbert-xlmr-{lang}\")\n"
      ],
      "metadata": {
        "id": "izqtox9dCAz9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}