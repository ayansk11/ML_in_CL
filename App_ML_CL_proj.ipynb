{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cpG7CV-OZAq"
      },
      "outputs": [],
      "source": [
        "# Installing core dependencies\n",
        "\n",
        "!pip install -q transformers[torch] datasets sentencepiece pyconll wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloning repositories\n",
        "\n",
        "!git clone -q https://github.com/lgessler/microbert"
      ],
      "metadata": {
        "id": "nt2rZlQ-O-8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the data\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, DatasetDict, ClassLabel\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForMaskedLM,\n",
        "    RobertaConfig, XLMRobertaConfig,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")"
      ],
      "metadata": {
        "id": "8ishSiB7PCOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_conllu_data(lang: str):\n",
        "    \"\"\"Load dataset with POS tags aligned with MicroBERT paper\"\"\"\n",
        "    base_path = Path(f\"microbert/data/{lang}/UD_*/\")\n",
        "    conll_files = list(base_path.glob(\"*.conllu\"))\n",
        "\n",
        "    label_feature = ClassLabel(names_file=f\"microbert/data/{lang}/upos_labels.txt\")\n",
        "\n",
        "    raw_datasets = DatasetDict({\n",
        "        split: load_dataset('conllu',\n",
        "                          data_files=str(file),\n",
        "                          split='train',\n",
        "                          features=Features({\n",
        "                              'tokens': Sequence(Value('string')),\n",
        "                              'upos': Sequence(label_feature)\n",
        "                          }))\n",
        "        for file, split in zip(conll_files, ['train', 'dev', 'test'])\n",
        "    })\n",
        "\n",
        "    return raw_datasets.map(\n",
        "        lambda ex: {'text': ' '.join(ex['tokens']), 'pos_tags': ex['upos']},\n",
        "        batched=True,\n",
        "        remove_columns=['id', 'lemma', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
        "    )"
      ],
      "metadata": {
        "id": "4LPloOEPtV7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MicroBERT(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, teacher, model_type='roberta'):\n",
        "        super().__init__()\n",
        "        self.config = self._get_config(teacher, model_type)\n",
        "        self.teacher = teacher\n",
        "        self.student = self._create_student()\n",
        "\n",
        "    def _get_config(self, teacher, model_type):\n",
        "        \"\"\"Create configuration matching MicroBERT paper\"\"\"\n",
        "        if model_type == 'roberta':\n",
        "\n",
        "            return RobertaConfig(\n",
        "                vocab_size=teacher.config.vocab_size,\n",
        "                hidden_size=512,\n",
        "                num_hidden_layers=6,\n",
        "                num_attention_heads=8,\n",
        "                intermediate_size=2048,\n",
        "                num_labels=len(ClassLabel(names_file=\"microbert/data/english/upos_labels.txt\"))\n",
        "            )\n",
        "\n",
        "        return XLMRobertaConfig(\n",
        "            vocab_size=teacher.config.vocab_size,\n",
        "            hidden_size=768,\n",
        "            num_hidden_layers=8,\n",
        "            num_attention_heads=12,\n",
        "            intermediate_size=3072,\n",
        "            num_labels=len(ClassLabel(names_file=\"microbert/data/english/upos_labels.txt\"))\n",
        "        )"
      ],
      "metadata": {
        "id": "nLqgY_WftXBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_student(self):\n",
        "  \"\"\"Create student with dual MLM + POS heads\"\"\"\n",
        "  model = AutoModelForMaskedLM.from_config(self.config)\n",
        "  model.pos_classifier = torch.nn.Linear(self.config.hidden_size, self.config.num_labels)\n",
        "  return model"
      ],
      "metadata": {
        "id": "ECx8WXs5tkkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_pos(examples, tokenizer):\n",
        "    \"\"\"Tokenize text while aligning POS tags to subwords\"\"\"\n",
        "    tokenized = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_offsets_mapping=True,\n",
        "        return_special_tokens_mask=True\n",
        "    )\n",
        "\n",
        "    pos_tags = []\n",
        "    for i, offsets in enumerate(tokenized['offset_mapping']):\n",
        "        word_ids = [idx for idx, (start, end) in enumerate(offsets) if start != end]\n",
        "        aligned_pos = [-100] * len(offsets)\n",
        "\n",
        "        for subword_idx, word_idx in enumerate(word_ids):\n",
        "            if word_idx is not None:\n",
        "                aligned_pos[subword_idx] = examples['pos_tags'][i][word_idx]\n",
        "\n",
        "        pos_tags.append(aligned_pos)\n",
        "\n",
        "    tokenized['pos_tags'] = pos_tags\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "3TVLKsBStqS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MicroBERTTrainer(Trainer):\n",
        "    def __init__(self, *args, alpha=0.7, temperature=4.0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        self.mse_loss = torch.nn.MSELoss()\n",
        "        self.ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Forward passes\n",
        "        student_outputs = model.student(**inputs)\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = model.teacher(\n",
        "                input_ids=inputs['input_ids'],\n",
        "                attention_mask=inputs['attention_mask']\n",
        "            )\n",
        "\n",
        "        # Loss calculations\n",
        "        mlm_loss = student_outputs.loss\n",
        "        pos_loss = self.ce_loss(\n",
        "            student_outputs.pos_logits.view(-1, model.config.num_labels),\n",
        "            inputs['pos_tags'].view(-1)\n",
        "        )\n",
        "\n",
        "        # Distillation losses\n",
        "        kl_loss = torch.nn.functional.kl_div(\n",
        "            torch.log_softmax(student_outputs.logits / self.temperature, dim=-1),\n",
        "            torch.softmax(teacher_outputs.logits / self.temperature, dim=-1),\n",
        "            reduction='batchmean'\n",
        "        ) * (self.temperature ** 2)\n",
        "\n",
        "        mse_loss = self.mse_loss(\n",
        "            student_outputs.hidden_states[-1],\n",
        "            teacher_outputs.hidden_states[-1]\n",
        "        )\n",
        "\n",
        "        # Combined loss (matches paper weights)\n",
        "        total_loss = (\n",
        "            0.3 * mlm_loss +\n",
        "            0.2 * pos_loss +\n",
        "            0.5 * (0.7 * kl_loss + 0.3 * mse_loss)\n",
        "        )\n",
        "\n",
        "        return (total_loss, student_outputs) if return_outputs else total_loss"
      ],
      "metadata": {
        "id": "nr5MKEyQttgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_microbert(lang: str, model_type='roberta'):\n",
        "    # Load data and models\n",
        "    dataset = load_conllu_data(lang)\n",
        "    teacher = AutoModelForMaskedLM.from_pretrained(\n",
        "        'xlm-roberta-base' if model_type == 'xlm' else 'roberta-base'\n",
        "    )\n",
        "    model = MicroBERT(teacher, model_type)\n",
        "\n",
        "    # Tokenization\n",
        "    tokenizer = AutoTokenizer.from_pretrained(teacher.name_or_path)\n",
        "    tokenized_ds = dataset.map(\n",
        "        lambda ex: tokenize_with_pos(ex, tokenizer),\n",
        "        batched=True,\n",
        "        batch_size=32\n",
        "    )"
      ],
      "metadata": {
        "id": "pUGu5ArTtz39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "        output_dir=f\"results/{lang}_{model_type}\",\n",
        "        learning_rate=5e-5,\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=32,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=100,\n",
        "        fp16=True,\n",
        "        report_to=\"wandb\"\n",
        "    )"
      ],
      "metadata": {
        "id": "3taTqmyKt2zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = MicroBERTTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tokenized_ds[\"train\"],\n",
        "        eval_dataset=tokenized_ds[\"dev\"],\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15),\n",
        "        alpha=0.7\n",
        "    )"
      ],
      "metadata": {
        "id": "UjXHG4Cft6bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "wkuyShkDt8j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and push to Hub\n",
        "    trainer.model.student.push_to_hub(\n",
        "        f\"microbert-{lang}-distilled\",\n",
        "        use_auth_token=\"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\"\n",
        "    )"
      ],
      "metadata": {
        "id": "-Np5hqnmt-jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\n",
        "        f\"microbert-{lang}-distilled\",\n",
        "        use_auth_token=\"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\"\n",
        "    )"
      ],
      "metadata": {
        "id": "Wj4SKDmDuAoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(lang: str, model_type: str):\n",
        "    # Load test data\n",
        "    test_ds = load_conllu_data(lang)['test']\n",
        "    model = AutoModelForMaskedLM.from_pretrained(f\"microbert-{lang}-distilled\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"microbert-{lang}-distilled\")\n",
        "\n",
        "    # POS Accuracy\n",
        "    inputs = tokenizer(test_ds['text'], truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        pos_preds = torch.argmax(outputs.pos_logits, dim=-1)\n",
        "\n",
        "    pos_accuracy = (pos_preds[inputs['attention_mask'] == 1] ==\n",
        "                   torch.tensor(test_ds['pos_tags'])[inputs['attention_mask'] == 1]).float().mean()\n",
        "\n",
        "    # Perplexity\n",
        "    eval_results = trainer.evaluate(tokenized_ds[\"test\"])\n",
        "    perplexity = np.exp(eval_results[\"eval_loss\"])\n",
        "\n",
        "    return {\n",
        "        'POS Accuracy': pos_accuracy.item(),\n",
        "        'Perplexity': perplexity\n",
        "    }"
      ],
      "metadata": {
        "id": "rZEK6hzVuCjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LANGUAGES = ['coptic', 'english', 'greek', 'greek_old', 'indonesian',\n",
        "             'latin', 'maltese', 'tamil', 'uyghur', 'wolof', 'wolof_old']\n",
        "\n",
        "results = []\n",
        "for lang in LANGUAGES:\n",
        "    # Train models\n",
        "    train_microbert(lang, 'roberta')\n",
        "    train_microbert(lang, 'xlm')\n",
        "\n",
        "    # Evaluate\n",
        "    roberta_metrics = evaluate_model(lang, 'roberta')\n",
        "    xlm_metrics = evaluate_model(lang, 'xlm')\n",
        "\n",
        "    results.append({\n",
        "        'Language': lang,\n",
        "        'RoBERTa POS Acc': roberta_metrics['POS Accuracy'],\n",
        "        'XLM-R POS Acc': xlm_metrics['POS Accuracy'],\n",
        "        'RoBERTa PPL': roberta_metrics['Perplexity'],\n",
        "        'XLM-R PPL': xlm_metrics['Perplexity']\n",
        "    })"
      ],
      "metadata": {
        "id": "P_YTGSFfuGXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('microbert_results.csv', index=False)\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "BBJzKUg9uKfo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}